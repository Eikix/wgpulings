# Exercise definitions for wgpulings - COMPUTE-FIRST EDITION
# Focused on parallel computing and GPU compute, with optional graphics at the end

# ============================================================================
# INTRO - GPU Compute Fundamentals
# ============================================================================

[[exercises]]
name = "intro01"
path = "exercises/intro/intro01.rs"
mode = "compile"
hint = """
GPU compute is fundamentally different from CPU programming:
- The GPU excels at PARALLEL processing (doing many things at once)
- You write programs (compute shaders) that run on thousands of threads simultaneously
- Data lives in GPU memory - you upload it, process it, then download results

For compute, we DON'T need:
- Windows or screens
- Rendering pipelines
- Graphics shaders

We DO need:
- Device and queue (connection to GPU)
- Compute shaders (WGSL programs)
- Buffers (GPU memory for data)

Think of it like: Upload data → Run parallel computation → Download results"""

[[exercises]]
name = "intro02"
path = "exercises/intro/intro02.rs"
mode = "run"
hint = """
Your first compute shader! This is the "Hello World" of GPU compute.

A compute shader:
- Is marked with @compute
- Has a workgroup size: @workgroup_size(X, Y, Z)
- Runs many instances in parallel
- Each instance has a unique ID: @builtin(global_invocation_id)

The workgroup size determines how many threads run together in a group.
Common sizes: (64), (8, 8), (256), etc.

Your shader will run on EVERY element of your data in parallel!"""

[[exercises]]
name = "intro03"
path = "exercises/intro/intro03.rs"
mode = "run"
hint = """
Buffers are chunks of memory on the GPU.

Key buffer types for compute:
- STORAGE: Read/write data (your main working memory)
- UNIFORM: Read-only parameters (constants for the computation)
- MAP_READ: Can be read from CPU (for getting results back)
- COPY_SRC/COPY_DST: Can be copied to/from

Typical workflow:
1. Create storage buffer with your data
2. Create staging buffer for results (MAP_READ)
3. Run compute shader
4. Copy storage → staging
5. Map staging buffer to read from CPU"""

[[exercises]]
name = "intro04"
path = "exercises/intro/intro04.rs"
mode = "run"
hint = """
Workgroups and threads are the key to parallel execution.

Hierarchy:
- Dispatch: Total number of workgroups (X, Y, Z)
- Workgroup: Group of threads that can cooperate (defined in shader)
- Thread: Individual execution unit

If you have 1000 elements and workgroup size of 64:
- Dispatch 16 workgroups (rounded up from 1000/64)
- Each workgroup runs 64 threads in parallel
- Total: 1024 thread invocations (some do nothing if > 1000)

Calculate: num_workgroups = ceil(num_elements / workgroup_size)"""

# ============================================================================
# COMPUTE BASICS - Core Parallel Programming
# ============================================================================

[[exercises]]
name = "basics01"
path = "exercises/basics/basics01.wgsl"
mode = "compile"
hint = """
WGSL (WebGPU Shading Language) for compute shaders.

Key syntax:
@compute @workgroup_size(64)
fn main(@builtin(global_invocation_id) global_id: vec3<u32>) {
    let index = global_id.x;
    // Your computation here
}

Storage buffers:
@group(0) @binding(0) var<storage, read_write> data: array<f32>;

Access: data[index] = value;

The global_invocation_id is unique for each thread - use it to index your data!"""

[[exercises]]
name = "basics02"
path = "exercises/basics/basics02.rs"
mode = "run"
hint = """
Storage buffers hold your data for computation.

Declaration in WGSL:
@group(0) @binding(0) var<storage, read> input: array<f32>;
@group(0) @binding(1) var<storage, read_write> output: array<f32>;

In Rust:
- Create buffer with BufferUsages::STORAGE
- Fill with data using queue.write_buffer() or create_buffer_init()
- Bind in a BindGroup matching your @group/@binding numbers

Access modes: <storage, read>, <storage, read_write>
Read-only is faster! Use read when you don't need to modify."""

[[exercises]]
name = "basics03"
path = "exercises/basics/basics03.rs"
mode = "run"
hint = """
Uniform buffers hold parameters/constants for your computation.

Uniforms:
- Small (up to 64KB typically)
- Read-only in shaders
- Fast access
- Great for parameters: size, scale, time, etc.

Example:
struct Params {
    scale: f32,
    offset: f32,
}
@group(0) @binding(2) var<uniform> params: Params;

Use uniforms for values that are constant across the whole computation!"""

[[exercises]]
name = "basics04"
path = "exercises/basics/basics04.rs"
mode = "run"
hint = """
Getting results back to CPU:

1. Compute writes to storage buffer
2. Copy storage buffer to staging buffer
3. Map staging buffer to CPU memory
4. Read the data!

Staging buffer requirements:
- BufferUsages::MAP_READ | BufferUsages::COPY_DST
- Can be mapped to CPU memory

Code pattern:
encoder.copy_buffer_to_buffer(storage, 0, staging, 0, size);
queue.submit(...);
let slice = staging.slice(..);
slice.map_async(...);
device.poll(...);
let data = slice.get_mapped_range();
// Read your results!"""

[[exercises]]
name = "basics05"
path = "exercises/basics/basics05.rs"
mode = "run"
hint = """
Multiple dispatches in one command buffer!

You can:
1. Run compute pass A
2. Run compute pass B (using A's output)
3. Run compute pass C
... all in one submission!

This avoids CPU-GPU round trips and is MUCH faster.

Pattern:
let mut encoder = device.create_command_encoder(...);
{
    let mut pass = encoder.begin_compute_pass(...);
    pass.set_pipeline(&pipeline1);
    pass.dispatch_workgroups(...);
    pass.set_pipeline(&pipeline2);
    pass.dispatch_workgroups(...);
}
queue.submit([encoder.finish()]);"""

# ============================================================================
# PARALLEL PATTERNS - Common Parallel Algorithms
# ============================================================================

[[exercises]]
name = "patterns01"
path = "exercises/patterns/patterns01.rs"
mode = "run"
hint = """
Parallel Map: Apply a function to every element independently.

This is the simplest parallel pattern!
- Each thread processes one element
- No communication between threads
- Perfect parallelism

Examples:
- Multiply every number by 2
- Convert RGB to grayscale
- Apply a function f(x) to array

Pattern:
let index = global_id.x;
output[index] = f(input[index]);

This is "embarrassingly parallel" - scales perfectly with more GPU cores!"""

[[exercises]]
name = "patterns02"
path = "exercises/patterns/patterns02.rs"
mode = "run"
hint = """
Parallel Reduction: Combine many values into one (e.g., sum, max, min).

This is trickier than map because threads need to COMBINE results.

Simple approach (two-pass):
1. Each workgroup reduces its chunk to one value
2. CPU reads partial results and combines them

Better approach (tree reduction):
1. Each thread loads one element
2. Threads cooperate in workgroup using shared memory
3. Tree-style reduction: pair up, add, repeat
4. Workgroup produces one result

For now, use the simple approach - we'll do tree reduction later!"""

[[exercises]]
name = "patterns03"
path = "exercises/patterns/patterns03.wgsl"
mode = "compile"
hint = """
Workgroup shared memory: Fast memory shared by threads in a workgroup.

Declaration:
var<workgroup> shared_data: array<f32, 256>;

Usage:
1. Each thread writes to shared memory
2. workgroupBarrier() - wait for all threads
3. Threads read from shared memory

This is MUCH faster than global memory for thread cooperation!

Important:
- Size must match workgroup size
- Need barriers for synchronization
- Only visible within one workgroup

Perfect for: reductions, scans, cooperative algorithms"""

[[exercises]]
name = "patterns04"
path = "exercises/patterns/patterns04.rs"
mode = "run"
hint = """
Parallel Scan (Prefix Sum): Each element gets sum of all previous elements.

Input:  [1, 2, 3, 4, 5]
Output: [1, 3, 6, 10, 15]

This is useful for:
- Compaction
- Allocation
- Radix sort
- Stream compaction

Algorithm (simplified two-pass):
1. Upsweep: Build partial sums in tree
2. Downsweep: Distribute sums back

For large arrays, you need multiple passes with workgroup-level scans.
Start simple: single workgroup scan using shared memory."""

[[exercises]]
name = "patterns05"
path = "exercises/patterns/patterns05.rs"
mode = "run"
hint = """
Histogram: Count occurrences of values in bins.

Example: Count how many pixels fall in each brightness range.

Challenge: Multiple threads may update the same bin (race condition!)

Solutions:
1. Atomic operations: atomicAdd(&histogram[bin], 1u)
2. Local histograms: Each workgroup builds its own, then merge
3. Striping: Partition data so threads don't collide

Atomics are simplest but can be slow if many threads hit the same bin.
For best performance, combine approach 2 (local) + 1 (atomic merge)."""

[[exercises]]
name = "patterns06"
path = "exercises/patterns/patterns06.wgsl"
mode = "compile"
hint = """
Atomic operations: Thread-safe read-modify-write operations.

Available atomics in WGSL:
- atomicAdd(ptr, value) - Add and return old value
- atomicSub(ptr, value) - Subtract
- atomicMax(ptr, value) - Max
- atomicMin(ptr, value) - Min
- atomicAnd/Or/Xor - Bitwise operations
- atomicExchange - Swap
- atomicCompareExchangeWeak - CAS

Storage must be:
- atomic<i32> or atomic<u32> (not f32!)
- In storage buffer or workgroup memory

Use for: histograms, counters, locks, synchronization"""

# ============================================================================
# PERFORMANCE - Optimization Techniques
# ============================================================================

[[exercises]]
name = "perf01"
path = "exercises/perf/perf01.rs"
mode = "run"
hint = """
Workgroup size matters for performance!

Too small (e.g., 1, 4):
- Underutilizes GPU
- More overhead

Too large (e.g., 1024):
- May not fit (hardware limits)
- Less flexibility for scheduler

Sweet spots: 64, 128, 256

Rules of thumb:
- Multiple of 32 (warp/wavefront size)
- Power of 2
- Consider your data size
- Benchmark!

Test different sizes and measure performance!"""

[[exercises]]
name = "perf02"
path = "exercises/perf/perf02.rs"
mode = "run"
hint = """
Memory coalescing: Access memory in patterns the GPU likes.

GOOD (coalesced):
Thread 0 reads index 0
Thread 1 reads index 1
Thread 2 reads index 2
→ One memory transaction!

BAD (scattered):
Thread 0 reads index 0
Thread 1 reads index 100
Thread 2 reads index 200
→ Three separate transactions!

Guideline: Adjacent threads should access adjacent memory.

Structure your algorithm so global_id.x maps to adjacent indices."""

[[exercises]]
name = "perf03"
path = "exercises/perf/perf03.rs"
mode = "run"
hint = """
Shared memory bank conflicts: How threads access shared memory matters.

GPU shared memory is organized in "banks" (typically 32).

GOOD (no conflict):
Thread 0 → bank 0
Thread 1 → bank 1
Thread 2 → bank 2

BAD (conflict):
Thread 0 → bank 0
Thread 1 → bank 0  ← CONFLICT! Serialized access

Avoid:
- Multiple threads accessing same bank
- Stride patterns that map to same bank

Solution: Pad shared memory arrays or change access pattern."""

[[exercises]]
name = "perf04"
path = "exercises/perf/perf04.rs"
mode = "run"
hint = """
Benchmarking GPU compute:

Use timestamps:
1. Create QuerySet with TIMESTAMP type
2. Write timestamp before compute
3. Write timestamp after compute
4. Resolve to buffer
5. Read and calculate delta

Important:
- Timestamp queries require TIMESTAMP_QUERY feature
- Multiple runs for accurate results
- Watch out for GPU warmup
- Measure end-to-end (include data transfer!)

Don't optimize blindly - measure first!"""

[[exercises]]
name = "perf05"
path = "exercises/perf/perf05.rs"
mode = "run"
hint = """
Occupancy: How many threads can run simultaneously on the GPU.

Limited by:
- Registers per thread
- Shared memory per workgroup
- Maximum threads per SM/CU

To increase occupancy:
- Reduce register usage (simpler shaders)
- Use less shared memory
- Adjust workgroup size

Higher occupancy → Better latency hiding → Better performance
(usually, but not always!)

Use profiling tools to see occupancy and identify bottlenecks."""

# ============================================================================
# APPLICATIONS - Real-World Use Cases
# ============================================================================

[[exercises]]
name = "app01"
path = "exercises/applications/app01.rs"
mode = "run"
hint = """
Image processing: Perfect for GPU parallel computing!

Each pixel is independent → Perfect parallelism

Common operations:
- Grayscale conversion
- Brightness/contrast
- Blur (each pixel reads neighbors)
- Edge detection
- Color correction

Pattern:
let x = global_id.x;
let y = global_id.y;
let index = y * width + x;
let color = input[index];
output[index] = process(color);

2D dispatch: dispatch_workgroups(width/8, height/8, 1)"""

[[exercises]]
name = "app02"
path = "exercises/applications/app02.rs"
mode = "run"
hint = """
Gaussian blur: Each pixel is the weighted average of neighbors.

Algorithm:
1. For each pixel (x, y)
2. Sample surrounding pixels in a radius
3. Multiply each by Gaussian weight
4. Sum and normalize

Challenge: Reading from neighbors (not coalesced perfectly)

Optimization: Two-pass separable blur
- Pass 1: Blur horizontally
- Pass 2: Blur vertically
This is faster and more cache-friendly!"""

[[exercises]]
name = "app03"
path = "exercises/applications/app03.rs"
mode = "run"
hint = """
Matrix multiplication: A classic parallel algorithm!

C[i,j] = sum over k of A[i,k] * B[k,j]

Naive approach:
- Each thread computes one element of C
- Loops over k dimension
- Slow: Many global memory accesses

Better approach (tiled):
1. Load tile of A and B into shared memory
2. Compute partial results
3. Repeat for next tiles
4. Accumulate results

This uses shared memory to reduce global memory traffic!"""

[[exercises]]
name = "app04"
path = "exercises/applications/app04.rs"
mode = "run"
hint = """
Particle simulation: Update many particles in parallel.

Each thread updates one particle:
- Apply forces
- Update velocity
- Update position
- Handle collisions/boundaries

Data structure:
struct Particle {
    position: vec2<f32>,
    velocity: vec2<f32>,
    mass: f32,
}

Perfect for GPU: Each particle independent (mostly)
Tricky part: Collisions between particles (need spatial partitioning)"""

[[exercises]]
name = "app05"
path = "exercises/applications/app05.rs"
mode = "run"
hint = """
N-body simulation: Each body affects all others (O(n²) problem).

Forces between bodies:
F = G * m1 * m2 / r²

Naive: Each thread computes forces from all other bodies

Optimizations:
1. Use shared memory to cache body data
2. Tiling: Process bodies in blocks
3. Barnes-Hut tree (more complex)
4. Fast Multipole Method (very complex)

Start with naive approach, then optimize with shared memory.
This is compute-heavy, so memory access patterns matter less."""

[[exercises]]
name = "app06"
path = "exercises/applications/app06.rs"
mode = "run"
hint = """
Sorting on GPU: Parallel sorting algorithms.

Good parallel sorts:
- Bitonic sort: O(log²n) comparisons, fully parallel
- Radix sort: O(n*k) where k is bits, uses scan
- Sample sort: Partition-based

Bitonic sort is simplest:
1. Sequences of increasing/decreasing order
2. Repeatedly merge and sort
3. Each stage is parallel compare-swap

For large data, radix sort is faster but more complex.
Start with bitonic sort for learning!"""

# ============================================================================
# GRAPHICS (Optional) - Visualization and Rendering
# ============================================================================

[[exercises]]
name = "graphics01"
path = "exercises/graphics/graphics01.rs"
mode = "run"
hint = """
[OPTIONAL] Graphics fundamentals: Rendering vs Compute

So far you've done COMPUTE (data processing).
Graphics adds VISUALIZATION (drawing results).

New concepts:
- Window/Surface: Where to draw
- Render pipeline: Vertex + Fragment shaders
- Vertex buffers: Geometry data
- Textures: Images to display

You can mix compute + graphics:
1. Compute shader processes data
2. Graphics pipeline visualizes it

This exercise sets up basic rendering."""

[[exercises]]
name = "graphics02"
path = "exercises/graphics/graphics02.rs"
mode = "run"
hint = """
[OPTIONAL] Visualizing compute results.

Pattern:
1. Run compute shader → writes to storage buffer
2. Bind storage buffer as vertex buffer
3. Render points/lines from the data

Or:
1. Compute writes to texture
2. Render texture to screen

This is powerful for:
- Particle visualization
- Scientific visualization
- Real-time data display
- Debugging compute shaders"""

[[exercises]]
name = "graphics03"
path = "exercises/graphics/graphics03.rs"
mode = "run"
hint = """
[OPTIONAL] Compute + render in one frame.

Animation loop:
1. Compute shader updates simulation
2. Render shader draws results
3. Repeat each frame

Storage buffers can be used by BOTH compute and render!

Ping-pong buffers:
- Frame N: Read from A, write to B
- Frame N+1: Read from B, write to A
- Avoids race conditions"""
